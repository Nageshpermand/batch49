{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n"
     ]
    }
   ],
   "source": [
    "s=range(1000)\n",
    "print(sys.getsizeof(4)*len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n"
     ]
    }
   ],
   "source": [
    "s=range(1000)\n",
    "print(sys.getsizeof(2)*len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "d=np.arange(1000)\n",
    "print(d.size*d.itemsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "d=np.arange(1000)\n",
    "print(d.size*d.itemsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named opencv2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a6b808b7047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopencv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named opencv2"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-91a2ffa00be2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named cv"
     ]
    }
   ],
   "source": [
    "import cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named opencv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-140d3f9c9adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopencv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named opencv"
     ]
    }
   ],
   "source": [
    "import opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "a=cv2.VideoCapture(0)\n",
    "while True:\n",
    "\t_, img=a.read()\n",
    "\tcv2.imshow(\" my face detection\",img)\n",
    "\tif cv2 . waitKey(1) & 0xFF==ord('q'):\n",
    "\t\tbreak\n",
    "a.release()\n",
    "#this program for face detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named _tkinter, please install the python-tk package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-546b2be457e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mturtle\u001b[0m \u001b[0;32mimport\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/turtle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m#print _ver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mTkinter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/Tkinter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', please install the python-tk package'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mtkinter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m \u001b[0;31m# b/w compat for export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mTclError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTclError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named _tkinter, please install the python-tk package"
     ]
    }
   ],
   "source": [
    "from turtle import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Tkinter as Tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import*\n",
    "forward(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import*\n",
    "write(0)\n",
    "forward(100)\n",
    "write(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'write' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7febfcac9f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'write' is not defined"
     ]
    }
   ],
   "source": [
    "write(0)\n",
    "forward(20)\n",
    "write(1)\n",
    "forward(20)\n",
    "write(2)\n",
    "forward(20)\n",
    "write(4)\n",
    "forward(20)\n",
    "write(5)\n",
    "forward(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(5):\n",
    "    write(step)\n",
    "    forward(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(6):\n",
    "    write(step)\n",
    "    forward(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import*\n",
    "goto(-140,140)\n",
    "\n",
    "for step in range(6):\n",
    "    write(step)\n",
    "    forward(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "penup()\n",
    "goto(-140,140)\n",
    "\n",
    "for step in range(6):\n",
    "    write(step)\n",
    "    forward(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(6):\n",
    "    right(90)\n",
    "    forward(10)\n",
    "    pendown()\n",
    "    forward(150)\n",
    "    penup()\n",
    "    backward(150)\n",
    "    left(90)\n",
    "    forward(20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import*\n",
    "\n",
    "speed(10)\n",
    "penup()\n",
    "goto(-140,140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Terminator",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminator\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d6ca5ab5f274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mada\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTurtle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'turtle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/turtle.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, undobuffersize, visible)\u001b[0m\n\u001b[1;32m   3705\u001b[0m                            \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3706\u001b[0m                            \u001b[0mundobuffersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mundobuffersize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3707\u001b[0;31m                            visible=visible)\n\u001b[0m\u001b[1;32m   3708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3709\u001b[0m \u001b[0mPen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTurtle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/turtle.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, canvas, shape, undobuffersize, visible)\u001b[0m\n\u001b[1;32m   2460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_undobuffersize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mundobuffersize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2461\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mundobuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mundobuffersize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2462\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/turtle.pyc\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2563\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2565\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drawturtle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2567\u001b[0m             \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# TurtleScreenBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/turtle.pyc\u001b[0m in \u001b[0;36m_update_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_incrementudc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updatecounter\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/turtle.pyc\u001b[0m in \u001b[0;36m_incrementudc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTurtleScreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RUNNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0mTurtleScreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RUNNING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTerminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updatecounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminator\u001b[0m: "
     ]
    }
   ],
   "source": [
    " forward(20)\n",
    "ada = Turtle()\n",
    "ada.color('red')\n",
    "ada.shape('turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Turtle.pendown of <turtle.Turtle object at 0x7f1be45363d0>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = Turtle()\n",
    "ada.color('red')\n",
    "ada.shape('turtle')\n",
    "\n",
    "ada.penup()\n",
    "ada.goto(-160, 100)\n",
    "ada.pendown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name randit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-058c60c95dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mturtle\u001b[0m \u001b[0;32mimport\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name randit"
     ]
    }
   ],
   "source": [
    "from turtle import*\n",
    "from random import randit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import*\n",
    "forward(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8e253df78dc4>, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8e253df78dc4>\"\u001b[0;36m, line \u001b[0;32m66\u001b[0m\n\u001b[0;31m    a=something for not execute  remove this line and get output\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# this is for facedetection\n",
    "import cv2\n",
    "\n",
    "# Method to generate dataset to recognize a person\n",
    "def generate_dataset(img, id, img_id):\n",
    "    # write image in data dir\n",
    "    cv2.imwrite(\"data/user.\"+str(id)+\".\"+str(img_id)+\".jpg\", img)\n",
    "\n",
    "# Method to draw boundary around the detected feature\n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text):\n",
    "    # Converting image to gray-scale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # detecting features in gray-scale image, returns coordinates, width and height of features\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    coords = []\n",
    "    # drawing rectangle around the feature and labeling it\n",
    "    for (x, y, w, h) in features:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(img, text, (x, y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        coords = [x, y, w, h]\n",
    "    return coords\n",
    "\n",
    "# Method to detect the features\n",
    "def detect(img, faceCascade, img_id):\n",
    "    color = {\"blue\":(255,0,0), \"red\":(0,0,255), \"green\":(0,255,0), \"white\":(255,255,255)}\n",
    "    coords = draw_boundary(img, faceCascade, 1.1, 10, color['blue'], \"Face\")\n",
    "    # If feature is detected, the draw_boundary method will return the x,y coordinates and width and height of rectangle else the length of coords will be 0\n",
    "    if len(coords)==4:\n",
    "        # Updating region of interest by cropping image\n",
    "        roi_img = img[coords[1]:coords[1]+coords[3], coords[0]:coords[0]+coords[2]]\n",
    "        # Assign unique id to each user\n",
    "        user_id = 1\n",
    "        # img_id to make the name of each image unique\n",
    "        generate_dataset(roi_img, user_id, img_id)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# Loading classifiers\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "# Capturing real time video stream. 0 for built-in web-cams, 0 or -1 for external web-cams\n",
    "video_capture = cv2.VideoCapture(-1)\n",
    "\n",
    "# Initialize img_id with 0\n",
    "img_id = 0\n",
    "\n",
    "while True:\n",
    "    if img_id % 50 == 0:\n",
    "        print(\"Collected \", img_id,\" images\")\n",
    "    # Reading image from video stream\n",
    "    _, img = video_capture.read()\n",
    "    # Call method we defined above\n",
    "    img = detect(img, faceCascade, img_id)\n",
    "    # Writing processed image in a new window\n",
    "    cv2.imshow(\"face detection\", img)\n",
    "    img_id += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# releasing web-cam\n",
    "video_capture.release()\n",
    "# Destroying output window\n",
    "cv2.destroyAllWindows()                           #observe last line  remove it and get output\n",
    "                                                 #observe last line  remove it and get output\n",
    "                            \n",
    "a=something for not execute  remove this line and get output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-77d3df5aa1b0>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-77d3df5aa1b0>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def generate_dataset(img, id, img_id):\n",
    "    \n",
    "    cv2.imwrite(\"data/user.\"+str(id)+\".\"+str(img_id)+\".jpg\", img)\n",
    "\n",
    "# this is for to draw boundary \n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text):\n",
    "    \n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    coords = []\n",
    "    \n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(img, text, (x, y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        coords = [x, y, w, h]\n",
    "    return coords\n",
    "\n",
    "\n",
    "def detect(img, faceCascade, eyecascade):\n",
    "    color = {\"blue\":(255,0,0), \"red\":(0,0,255), \"green\":(0,255,0), \"white\":(255,255,255)}\n",
    "    coords = draw_boundary(img, faceCascade, 1.1, 10, color['green'], \"Nag\")\n",
    "     \n",
    "    if len(coords)==4:\n",
    "        \n",
    "        roi_img = img[coords[1]:coords[1]+coords[3], coords[0]:coords[0]+coords[2]]\n",
    "        coords=draw_boundary(roi_img, eyecascade, 1.1, 14, color['red'],\"eyes\")\n",
    "       \n",
    "        \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eyesCascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "\n",
    "\n",
    "video_capture = cv2.VideoCapture(-1)\n",
    "\n",
    "\n",
    "img_id = 0\n",
    "\n",
    "while True:\n",
    "    if img_id % 50 == 0:\n",
    "        print(\"Collected \", img_id,\" images\")\n",
    "    # Reading image from video \n",
    "    _, img = video_capture.read()\n",
    "    \n",
    "    img = detect(img, faceCascade, eyesCascade)\n",
    "    \n",
    "    cv2.imshow(\"face detection\", img)\n",
    "    img_id += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "                                               #observe last line  remove it and get output\n",
    "                                                 #observe last line  remove it and get output\n",
    "                            \n",
    "a=something for not execute  remove this line and get output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'CV_FONT_HERSHEY_SIMPLEX'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f5ab3b26ee95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"kcr.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Hello World!!!\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCV_FONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'CV_FONT_HERSHEY_SIMPLEX'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "img=\"kcr.jpg\"\n",
    "cv2.putText(img,\"Hello World!!!\",  cv2.CV_FONT_HERSHEY_SIMPLEX, 2, 255)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Collected ', 0, ' images')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'detect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c054f77e6630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Call method we defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m      \u001b[0;31m# Writing processed image in a new window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detect' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Method to generate dataset to recognize a human face\n",
    "def generate_dataset(img, id, img_id):\n",
    "     # write image in data direction\n",
    "    cv2.imwrite(\"data/user.%s\"%(id)+\".\"%(img_id)+\".jpg\", img)\n",
    "\n",
    "# this is for to draw boundary \n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text):\n",
    "    # Converting image to gray-scale (grayscale image contains only shades of gray and no color)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detecting features in gray-scale image, returns coordinates, width and height of features\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    coords = []\n",
    "\n",
    "    # drawing rectangle around the feature and labeling it\n",
    "    for (x, y, w, h) in features:\n",
    "       cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "       cv2.putText(img, text, (x, y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "       coords = [x, y, w, h]\n",
    "    #Coordinates are a set of values thatshowan exact position.On graphs it iscommon to have a pairofnumbers to show\n",
    "    return coords\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "img_id = 0\n",
    "\n",
    "while True:\n",
    "    if img_id % 50 == 0:\n",
    "        print(\"Collected \", img_id,\" images\")\n",
    "    # Reading image from video \n",
    "    _, img = video_capture.read()\n",
    "    \n",
    "    # Call method we defined above\n",
    "    img = detect(img)\n",
    "    \n",
    "     # Writing processed image in a new window\n",
    "    cv2.imshow(\"face detection\", img)\n",
    "    img_id += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rects(img, rects):\n",
    "    \"\"\"\n",
    "    ?????????????\n",
    "    :param img: \n",
    "    :param rects: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    for x, y, w, h in rects:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 255, 00), 2)\n",
    "        face = img\n",
    "        face = cv2.resize(face,(224,224))\n",
    "        if Gender.predict(face)==1:\n",
    "            text = \"Male\"\n",
    "        else:\n",
    "            text = \"Female\"\n",
    "        cv2.putText(img, text, (x, h), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), lineType=cv2.LINE_AA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-2128f2008a5b>, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-2128f2008a5b>\"\u001b[0;36m, line \u001b[0;32m85\u001b[0m\n\u001b[0;31m    a=something for not execute  remove this line and get output\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# open cv2 is computer vision open sourse library files are available\n",
    "# this is for face and eyes detection\n",
    "import cv2\n",
    "\n",
    "# Method to generate dataset to recognize a human face\n",
    "def generate_dataset(img, id, img_id):\n",
    "     # write image in data direction\n",
    "    cv2.imwrite(\"data/user.%s\"%(id)+\".\"%(img_id)+\".jpg\", img)\n",
    "\n",
    "# this is for to draw boundary \n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text):\n",
    "    # Converting image to gray-scale (grayscale image contains only shades of gray and no color)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detecting features in gray-scale image, returns coordinates, width and height of features\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    coords = []\n",
    "\n",
    "    # drawing rectangle around the feature and labeling it\n",
    "    for (x, y, w, h) in features:\n",
    "       cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "       cv2.putText(img, text, (x, y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "       coords = [x, y, w, h]\n",
    "    #Coordinates are a set of values thatshowan exact position.On graphs it iscommon to have a pairofnumbers to show\n",
    "\n",
    "    return coords\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Method to detect the features  and (255,0,0)and(0,0,255) rgb values\n",
    "def detect(img, faceCascade, eyecascade):\n",
    "    color = {\"blue\":(255,0,0), \"red\":(0,0,255), \"green\":(0,255,0), \"white\":(255,255,255)}\n",
    "    coords = draw_boundary(img, faceCascade, 1.1, 10, color['green'], \"Nag\")\n",
    "    \n",
    "\n",
    "     #If feature is detected, the draw_boundary method will returnthe x,y coordinatesand width andheight of rectangle\n",
    "                \n",
    "    if len(coords)==4:\n",
    "        \n",
    "         # Updating region by cropping image\n",
    "         #roi_img = a region of intrest which is a binary img that is the same size as the image want to process\n",
    "         # and roi img can createROIs ofmany shapes using the high-level ROI functions,such asdrawcircle ordrawpolygon\n",
    "        roi_img = img[coords[1]:coords[1]+coords[3], coords[0]:coords[0]+coords[2]]\n",
    "                # Assign unique id to each user\n",
    "\n",
    "        coords=draw_boundary(roi_img, eyecascade, 1.1, 14, color['red'],\"eyes\")\n",
    "       \n",
    "        \n",
    "    return img\n",
    "\n",
    "\n",
    "# Loading classifiers facecascade and eyescascade\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eyesCascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Capturing real time video stream. 0 for built-in web-cams, 0 or -1 for external web-cams\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "img_id = 0\n",
    "\n",
    "while True:\n",
    "    if img_id % 50 == 0:\n",
    "        print(\"Collected \", img_id,\" images\")\n",
    "    # Reading image from video \n",
    "    _, img = video_capture.read()\n",
    "    \n",
    "    # Call method we defined above\n",
    "    img = detect(img, faceCascade, eyesCascade)\n",
    "    \n",
    "     # Writing processed image in a new window\n",
    "    cv2.imshow(\"face detection\", img)\n",
    "    img_id += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# releasing  machine-cam\n",
    "video_capture.release()                 #observe last line  remove it and get output\n",
    "                                                 #observe last line  remove it and get output\n",
    "                            \n",
    "a=something for not execute  remove this line and get output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
